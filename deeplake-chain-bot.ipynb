{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY =os.getenv(\"OPENAI_API_KEY\")\n",
    "ACTIVELOOP_TOKEN=os.getenv(\"ACTIVELOOP_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import OpenAI \n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https:/app.activeloop.ai/pravosnazna/dataset101 loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in https:/app.activeloop.ai/pravosnazna/dataset101 already exists, loading from the storage\n",
      "Dataset(path='https:/app.activeloop.ai/pravosnazna/dataset101', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype    shape    dtype  compression\n",
      "  -------   -------  -------  -------  ------- \n",
      " embedding  generic   (0,)    float32   None   \n",
      "    ids      text     (0,)      str     None   \n",
      " metadata    json     (0,)      str     None   \n",
      "   text      text     (0,)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|██████████| 1/1 [00:05<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='https:/app.activeloop.ai/pravosnazna/dataset101', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape     dtype  compression\n",
      "  -------   -------   -------   -------  ------- \n",
      " embedding  generic  (2, 1536)  float32   None   \n",
      "    ids      text     (2, 1)      str     None   \n",
      " metadata    json     (2, 1)      str     None   \n",
      "   text      text     (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#DEEPLAKE_DATASET_PATH='hub://pravosnazna/dataset101'\n",
    "dataset_path='https:/app.activeloop.ai/pravosnazna/dataset101'\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "def get_documentation_urls():\n",
    "    # List of relative URLs for Hugging Face documentation pages, commented a lot of these because it would take too long to scrape all of them\n",
    "    return [\n",
    "    'https://zapier.com/resources/guides/quick-start/automation-basics',\n",
    "    'https://zapier.com/resources/guides/quick-start/create-zap',\n",
    "    'https://zapier.com/resources/guides/quick-start/more-to-know',\n",
    "    'https://zapier.com/blog/maximize-productivity-with-multi-step-zaps/',\n",
    "    'https://zapier.com/blog/how-ai-tools-reach-no-code-audiences/',\n",
    "    'https://zapier.com/blog/new-product-features-april-2023/',\n",
    "    \n",
    "    ]\n",
    "\n",
    "\n",
    "def construct_full_url(base_url, relative_url):\n",
    "    # Construct the full URL by appending the relative URL to the base URL\n",
    "    return base_url + relative_url\n",
    "\n",
    "\n",
    "def scrape_page_content(url):\n",
    "    # Send a GET request to the URL and parse the HTML response using BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract the desired content from the page (in this case, the body text)\n",
    "    text=soup.body.text.strip()\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def scrape_all_content(base_url, relative_urls, filename):\n",
    "    # Loop through the list of URLs, scrape content, and add it to the content list\n",
    "    content = []\n",
    "    for relative_url in relative_urls:\n",
    "        full_url = construct_full_url(base_url, relative_url)\n",
    "        scraped_content = scrape_page_content(full_url)\n",
    "        content.append(scraped_content.rstrip('\\n'))\n",
    "\n",
    "    # Write the scraped content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for item in content:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Define a function to load documents from a file\n",
    "def load_docs(root_dir,filename):\n",
    "    # Create an empty list to hold the documents\n",
    "    docs = []\n",
    "    try:\n",
    "        # Load the file using the TextLoader class and UTF-8 encoding\n",
    "        loader = TextLoader(os.path.join(\n",
    "            root_dir, filename), encoding='utf-8')\n",
    "        # Split the loaded file into separate documents and add them to the list of documents\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        # If an error occurs during loading, ignore it and return an empty list of documents\n",
    "        pass\n",
    "    # Return the list of documents\n",
    "    return docs\n",
    "\n",
    "  \n",
    "def split_docs(docs):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(docs) \n",
    "\n",
    "def load_vectors_into_deeplake(dataset_path, source_chunks):\n",
    "    # Initialize the DeepLake database with the dataset path and embedding function\n",
    "    deeplake_db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "    # Add the text chunks to the database\n",
    "    deeplakedb=deeplake_db.add_texts(source_chunks)\n",
    "    return deeplakedb\n",
    "\n",
    "\n",
    "# Define the main function\n",
    "def main():\n",
    "    base_url = 'https://zapier.com/'\n",
    "    # Set the name of the file to which the scraped content will be saved\n",
    "    filename='content.txt'\n",
    "    # Set the root directory where the content file will be saved\n",
    "    root_dir =''\n",
    "    relative_urls = get_documentation_urls()\n",
    "    # Scrape all the content from the relative urls and save it to the content file\n",
    "    content = scrape_all_content(base_url, relative_urls,filename)\n",
    "    # Load the content from the file\n",
    "    docs = load_docs(root_dir,filename)\n",
    "    # Split the content into individual documents\n",
    "    texts = split_docs(docs)\n",
    "    # Create a DeepLake database with the given dataset path and embedding function\n",
    "    db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "    # Add the individual documents to the database\n",
    "    db.add_documents(texts)\n",
    "    # Clean up by deleting the content file\n",
    "    os.remove(filename)\n",
    "\n",
    "# Call the main function if this script is being run as the main program\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https:/app.activeloop.ai/pravosnazna/dataset101 loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in https:/app.activeloop.ai/pravosnazna/dataset101 already exists, loading from the storage\n",
      "Dataset(path='https:/app.activeloop.ai/pravosnazna/dataset101', read_only=True, tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype    shape    dtype  compression\n",
      "  -------   -------  -------  -------  ------- \n",
      " embedding  generic   (0,)    float32   None   \n",
      "    ids      text     (0,)      str     None   \n",
      " metadata    json     (0,)      str     None   \n",
      "   text      text     (0,)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Prepare a retriever from the deeplake_db\n",
    "db = DeepLake(dataset_path=dataset_path, read_only=True, embedding_function=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 20\n",
    "# Construct the prompt template for the chain\n",
    "template = \"\"\"\n",
    "Given these portions of documents and the presented question, you're tasked to provide a comprehensive response with appropriate sources. If the answer is beyond my capabilities, I'll acknowledge that instead of guessing. Every response should include relevant sources.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "DOCUMENT EXCERPTS:\n",
    "{summaries}\n",
    "\n",
    "RESPONSE AND SOURCES:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"summaries\", \"question\"],\n",
    ")\n",
    "\n",
    "# Construct the RetrievalQAWithSourcesChain\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt_template,\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Zapier?',\n",
       " 'answer': '\\nZapier is an online automation tool that allows users to connect different web applications and automate repetitive tasks. It is a web-based service that enables users to integrate the web applications they use, and automate workflows without any coding. Zapier allows users to create \"Zaps\" which are automated workflows that connect two or more apps together. For example, a user can create a Zap that automatically adds new contacts from a spreadsheet to their CRM. Zapier is used by millions of people around the world to automate their workflows and save time. \\n\\nSources: \\n\\n- https://zapier.com/\\n- https://www.g2.com/products/zapier/reviews\\n- https://www.capterra.com/p/170045/Zapier/',\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"What is Zapier?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "As a Question Answering Chatbot, I am tasked to provide comprehensive answers to specific questions based on the provided document excerpts. If the information is not contained within these excerpts, it is crucial to admit the absence of information rather than speculate. Each answer MUST include reference to the sources it was derived from. Please ensure that all answers end with a 'SOURCES' section detailing from where the information was obtained.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "DOCUMENT EXCERPTS:\n",
    "{summaries}\n",
    "\n",
    "RESPONSE:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"summaries\", \"question\"],\n",
    ")\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\", # Replace with the correct chain type\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt_template,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How Zapier connects two apps together?',\n",
       " 'answer': 'Zapier is a web-based automation tool that connects two apps together. It allows users to create automated workflows, called Zaps, that can be triggered by a specific event in one app and then perform an action in another app. For example, a Zap can be set up to automatically create a new task in a project management app when a new customer is added to a CRM. ',\n",
       " 'sources': 'https://zapier.com/learn/getting-started-guide/what-is-zapier/'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"How Zapier connects two apps together?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"https://gpt-index.readthedocs.io/en/latest/index.html\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Getting Started', 'Guides', 'Use Cases', 'Key Components', 'Reference', 'Gallery', 'LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM‚Äôs with external data.', 'Github: https://github.com/jerryjliu/llama_index', 'LlamaIndex: https://pypi.org/project/llama-index/.', 'GPT Index (duplicate): https://pypi.org/project/gpt-index/.', 'Twitter: https://twitter.com/gpt_index', 'Discord https://discord.gg/dGcwcsnxhU', 'LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.', 'How do we best augment LLMs with our own private data?', 'One paradigm that has emerged is in-context learning (the other is finetuning), where we insert context into the input prompt. That way, we take advantage of the LLM‚Äôs reasoning capabilities to generate a response.', 'To perform LLM‚Äôs data augmentation in a performant, efficient, and cheap manner, we need to solve two components:', 'Data Ingestion', 'Data Indexing', 'That‚Äôs where the LlamaIndex comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion:', 'Offers data connectors to your existing data sources and data formats (API‚Äôs, PDF‚Äôs, docs, SQL, etc.)', 'Provides indices over your unstructured and structured data for use with LLM‚Äôs. These indices help to abstract away common boilerplate and pain points for in-context learning:', 'Storing context in an easy-to-access format for prompt insertion.', 'Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big.', 'Dealing with text splitting.', 'Provides users an interface to query the index (feed in an input prompt) and obtain a knowledge-augmented output.', 'Offers you a comprehensive toolset trading off cost and performance.', 'Getting Started', 'Guides', 'Use Cases', 'Key Components', 'Reference', 'Gallery']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get the HTML content from the GPT Index documentation page\n",
    "url = 'https://gpt-index.readthedocs.io/en/latest/index.html'\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content with Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the paragraphs on the page\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "# Store the text content of each paragraph in a list\n",
    "content = []\n",
    "for paragraph in paragraphs:\n",
    "    content.append(paragraph.get_text())\n",
    "\n",
    "# Print the list of content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents Menu Expand Light mode Dark mode Auto light/dark mode Hide navigation sidebar Hide table of contents sidebar Toggle site navigation sidebar LlamaIndex Toggle Light / Dark / Auto color theme Toggle table of contents sidebar LlamaIndex Getting Started Installation and Setup Starter Tutorial Guides A Primer to using LlamaIndexToggle child pages in navigation LlamaIndex Usage Pattern How Each Index Works TutorialsToggle child pages in navigation üí¨ü§ñ How to Build a Chatbot A Guide to Building a Full-Stack Web App with LLamaIndex A Guide to LlamaIndex + Structured Data A Guide to Extracting Terms and Definitions A Guide to Creating a Unified Query Framework over your Indexes SEC 10k Analysis Notebooks Use Cases Queries over your Data Integrations into LLM Applications Key Components Data Connectors (LlamaHub ü¶ô) Index StructuresToggle child pages in navigation How Each Index Works Updating an Index Composability Query InterfaceToggle child pages in navigation LlamaIndex Usage Pattern Composability Query Transformations Node Postprocessor CustomizationToggle child pages in navigation Defining LLMs Defining Prompts Embedding support Analysis and OptimizationToggle child pages in navigation Cost Analysis Playground Optimizers Output Parsing üî¨ Evaluation IntegrationsToggle child pages in navigation Using Vector Stores ChatGPT Plugin Integrations Using with Langchain ü¶úüîó Document Store Reference IndicesToggle child pages in navigation List Index Table Index Tree Index Vector Store IndexToggle child pages in navigation Vector Stores Base Vector Index class Structured Store Index Knowledge Graph Index Empty Index Querying an IndexToggle child pages in navigation Querying a List Index Querying a Keyword Table Index Querying a Tree Index Querying a Vector Store Index Querying a Structured Store Index Querying a Knowledge Graph Index Querying an Empty Index Composable Queries Node Node Postprocessor Docstore Composability Data Connectors Prompt Templates Service ContextToggle child pages in navigation Embeddings LLMPredictor PromptHelper Llama Logger ü™µ Optimizers Structured Index Configuration Response Playground Node Parser Example Notebooks Langchain Integrations Gallery üòé App Showcase v: latest Versions latest stable Downloads pdf html epub On Read the Docs Project Home Builds Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Welcome to LlamaIndex ü¶ô !ÔÉÅ LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM‚Äôs with external data. Github: https://github.com/jerryjliu/llama_index PyPi: LlamaIndex: https://pypi.org/project/llama-index/. GPT Index (duplicate): https://pypi.org/project/gpt-index/. Twitter: https://twitter.com/gpt_index Discord https://discord.gg/dGcwcsnxhU üöÄ OverviewÔÉÅ ContextÔÉÅ LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data. How do we best augment LLMs with our own private data? One paradigm that has emerged is in-context learning (the other is finetuning), where we insert context into the input prompt. That way, we take advantage of the LLM‚Äôs reasoning capabilities to generate a response. To perform LLM‚Äôs data augmentation in a performant, efficient, and cheap manner, we need to solve two components: Data Ingestion Data Indexing Proposed SolutionÔÉÅ That‚Äôs where the LlamaIndex comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion: Offers data connectors to your existing data sources and data formats (API‚Äôs, PDF‚Äôs, docs, SQL, etc.) Provides indices over your unstructured and structured data for use with LLM‚Äôs. These indices help to abstract away common boilerplate and pain points for in-context learning: Storing context in an easy-to-access format for prompt insertion. Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big. Dealing with text splitting. Provides users an interface to query the index (feed in an input prompt) and obtain a knowledge-augmented output. Offers you a comprehensive toolset trading off cost and performance. Getting Started Installation and Setup Starter Tutorial Guides A Primer to using LlamaIndex LlamaIndex Usage Pattern How Each Index Works Tutorials üí¨ü§ñ How to Build a Chatbot A Guide to Building a Full-Stack Web App with LLamaIndex A Guide to LlamaIndex + Structured Data A Guide to Extracting Terms and Definitions A Guide to Creating a Unified Query Framework over your Indexes SEC 10k Analysis Notebooks Use Cases Queries over your Data Semantic Search Summarization Queries over Structured Data Synthesis over Heterogenous Data Routing over Heterogenous Data Compare/Contrast Queries Multi-Step Queries Integrations into LLM Applications Chatbots Full-Stack Web Application Key Components Data Connectors (LlamaHub ü¶ô) Index Structures Query Interface Customization Analysis and Optimization Output Parsing üî¨ Evaluation Integrations Document Store Reference Indices Querying an Index Node Node Postprocessor Docstore Composability Data Connectors Prompt Templates Service Context Optimizers Structured Index Configuration Response Playground Node Parser Example Notebooks Langchain Integrations Gallery üòé App Showcase Next Installation and Setup Copyright ¬© 2022, Jerry Liu Made with Sphinx and @pradyunsg's Furo On this page Welcome to LlamaIndex ü¶ô ! üöÄ Overview Context Proposed Solution\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import html\n",
    "import re\n",
    "import requests\n",
    "\n",
    "res = requests.get(\"https://gpt-index.readthedocs.io/en/latest/\")\n",
    "domain = \"https://gpt-index.readthedocs.io/en/latest/\"\n",
    "domain_full = domain\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# Find all links to local pages on the website\n",
    "local_links = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    href = link['href']\n",
    "    if href.startswith(domain) or href.startswith('./') \\\n",
    "        or href.startswith('/') or href.startswith('modules') \\\n",
    "        or href.startswith('use_cases'):\n",
    "        local_links.append(urllib.parse.urljoin(domain_full, href))\n",
    "\n",
    "# Find the main content using CSS selectors\n",
    "main_content = soup.select('body')[0]\n",
    "\n",
    "# Extract the HTML code of the main content\n",
    "main_content_html = str(main_content)\n",
    "\n",
    "# Extract the plaintext of the main content\n",
    "main_content_text = main_content.get_text()\n",
    "\n",
    "# Remove all HTML tags\n",
    "main_content_text = re.sub(r'<[^>]+>', '', main_content_text)\n",
    "\n",
    "# Remove extra white space\n",
    "main_content_text = ' '.join(main_content_text.split())\n",
    "\n",
    "# Replace HTML entities with their corresponding characters\n",
    "main_content_text = html.unescape(main_content_text)\n",
    "\n",
    "print(main_content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to scrape {url}\")\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    main_content = soup.select_one('.document .body .section')\n",
    "    if main_content is None:\n",
    "        print(f\"No content found in {url}\")\n",
    "        return None\n",
    "    page_content = main_content.get_text()\n",
    "    local_links = [link.get('href') for link in main_content.find_all('a') if link.get('href').startswith('/')]\n",
    "    local_links = [urljoin(response.url, link) for link in local_links]\n",
    "    return page_content, local_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('p50k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for idx, record in enumerate(tqdm(data)):\n",
    "    texts = text_splitter.split_text(record['text'])\n",
    "    chunks.extend([{\n",
    "        'id': str(uuid4()),\n",
    "        'text': texts[i],\n",
    "        'chunk': i,\n",
    "        'url': record['url']\n",
    "    } for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-ada-002\"\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[\n",
    "        \"Sample document text goes here\",\n",
    "        \"there will be several phrases in each batch\"\n",
    "    ], engine=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['object', 'data', 'model', 'usage'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 1536)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv(\"22e6337a-1ab8-49c2-9ac4-281f09e6bde5\") \n",
    "# find your environment next to the api key in pinecone console\n",
    "env = os.getenv(\"us-west1-gcp\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloudsuperadmin/.local/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=\"22e6337a-1ab8-49c2-9ac4-281f09e6bde5\",  # app.pinecone.io\n",
    "    environment=\"us-west1-gcp\"  # next to API key in console\n",
    ")\n",
    "\n",
    "index_name = \"globaltelbot\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    raise ValueError(\n",
    "        f\"No '{index_name}' index exists. You must create the index before \"\n",
    "        \"running this notebook. Please refer to the walkthrough at \"\n",
    "        \"'github.com/pinecone-io/examples'.\"  # TODO add full link\n",
    "    )\n",
    "\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(len(chunks), i+batch_size)\n",
    "    meta_batch = chunks[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = [x['id'] for x in meta_batch]\n",
    "    # get texts to encode\n",
    "    texts = [x['text'] for x in meta_batch]\n",
    "    # create embeddings (try-except added to avoid RateLimitError)\n",
    "    try:\n",
    "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "    except:\n",
    "        done = False\n",
    "        while not done:\n",
    "            sleep(5)\n",
    "            try:\n",
    "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "                done = True\n",
    "            except:\n",
    "                pass\n",
    "    embeds = [record['embedding'] for record in res['data']]\n",
    "    # cleanup metadata\n",
    "    meta_batch = [{\n",
    "        'text': x['text'],\n",
    "        'chunk': x['chunk'],\n",
    "        'url': x['url']\n",
    "    } for x in meta_batch]\n",
    "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"can I use llamaindex with langchain?\"\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[query],\n",
    "    engine=embed_model\n",
    ")\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res['data'][0]['embedding']\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(xq, top_k=5, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of retrieved text\n",
    "contexts = [item['metadata']['text'] for item in res['matches']]\n",
    "\n",
    "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Application. Some of these applications are described below. ChatbotsÔÉÅ Chatbots are an incredibly popular use case for LLM‚Äôs. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents. Relevant Resources: Building a Chatbot Using with a LangChain Agent Full-Stack Web ApplicationÔÉÅ LlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit. We provide tutorials and resources to help you get started in this area. Relevant Resources: Fullstack Application Guide LlamaIndex Starter Pack Next Data Connectors (LlamaHub ü¶ô) Previous Queries over your Data Copyright ¬© 2022, Jerry Liu Made with Sphinx and @pradyunsg's Furo On this page Integrations into LLM Applications Chatbots Full-Stack Web Application\n",
      "\n",
      "---\n",
      "\n",
      "connect your LLM‚Äôs with external data. Github: https://github.com/jerryjliu/llama_index PyPi: LlamaIndex: https://pypi.org/project/llama-index/. GPT Index (duplicate): https://pypi.org/project/gpt-index/. Twitter: https://twitter.com/gpt_index Discord https://discord.gg/dGcwcsnxhU üöÄ OverviewÔÉÅ ContextÔÉÅ LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data. How do we best augment LLMs with our own private data? One paradigm that has emerged is in-context learning (the other is finetuning), where we insert context into the input prompt. That way, we take advantage of the LLM‚Äôs reasoning capabilities to generate a response. To perform LLM‚Äôs data augmentation in a performant, efficient, and cheap manner, we need to solve two components: Data Ingestion Data Indexing Proposed SolutionÔÉÅ That‚Äôs where the LlamaIndex comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion: Offers data connectors to your existing data sources and data formats (API‚Äôs, PDF‚Äôs, docs, SQL, etc.)\n",
      "\n",
      "---\n",
      "\n",
      "connect your LLM‚Äôs with external data. Github: https://github.com/jerryjliu/llama_index PyPi: LlamaIndex: https://pypi.org/project/llama-index/. GPT Index (duplicate): https://pypi.org/project/gpt-index/. Twitter: https://twitter.com/gpt_index Discord https://discord.gg/dGcwcsnxhU üöÄ OverviewÔÉÅ ContextÔÉÅ LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data. How do we best augment LLMs with our own private data? One paradigm that has emerged is in-context learning (the other is finetuning), where we insert context into the input prompt. That way, we take advantage of the LLM‚Äôs reasoning capabilities to generate a response. To perform LLM‚Äôs data augmentation in a performant, efficient, and cheap manner, we need to solve two components: Data Ingestion Data Indexing Proposed SolutionÔÉÅ That‚Äôs where the LlamaIndex comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion: Offers data connectors to your existing data sources and data formats (API‚Äôs, PDF‚Äôs, docs, SQL, etc.)\n",
      "\n",
      "---\n",
      "\n",
      "LLM Applications Chatbots Full-Stack Web Application Key Components Data Connectors (LlamaHub ü¶ô) Index Structures Query Interface Customization Analysis and Optimization Output Parsing üî¨ Evaluation Integrations Document Store Reference Indices Querying an Index Node Node Postprocessor Docstore Composability Data Connectors Prompt Templates Service Context Optimizers Structured Index Configuration Response Playground Node Parser Example Notebooks Langchain Integrations Gallery üòé App Showcase Next Installation and Setup Copyright ¬© 2022, Jerry Liu Made with Sphinx and @pradyunsg's Furo On this page Welcome to LlamaIndex ü¶ô ! üöÄ Overview Context Proposed Solution\n",
      "\n",
      "---\n",
      "\n",
      "LLM Applications Chatbots Full-Stack Web Application Key Components Data Connectors (LlamaHub ü¶ô) Index Structures Query Interface Customization Analysis and Optimization Output Parsing üî¨ Evaluation Integrations Document Store Reference Indices Querying an Index Node Node Postprocessor Docstore Composability Data Connectors Prompt Templates Service Context Optimizers Structured Index Configuration Response Playground Node Parser Example Notebooks Langchain Integrations Gallery üòé App Showcase Next Installation and Setup Copyright ¬© 2022, Jerry Liu Made with Sphinx and @pradyunsg's Furo On this page Welcome to LlamaIndex ü¶ô ! üöÄ Overview Context Proposed Solution\n",
      "\n",
      "-----\n",
      "\n",
      "can I use llamaindex with langchain?\n"
     ]
    }
   ],
   "source": [
    "print(augmented_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system message \n",
    "template = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
    "user questions based on the information provided by the user above\n",
    "each question. If the information can not be found in the information\n",
    "provided by the user you truthfully say \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": template},\n",
    "        {\"role\": \"user\", \"content\": augmented_query}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, according to the information provided, LlamaIndex can be used to build Knowledge-augmented chatbots and agents using LangChain. The documentation also includes a tutorial on how to build a chatbot with LangChain."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(res['choices'][0]['message']['content']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
